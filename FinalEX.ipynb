{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Exercise\n",
    "This is the final exercise in our Into to Machine Learning Course.\\\n",
    "207027053 - Gil Yair Yamin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing we need to do is of course import all libraries we will be using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have here a csv file with data about cancer patients.\n",
    "Using numpy, lets import the data from the csv we were given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = np.loadtxt(\"cancer_data.csv\", dtype=np.longdouble, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to normalize our data to continue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(raw_data):\n",
    "    res = np.zeros_like(raw_data, dtype=np.longdouble)\n",
    "    ExpectedArr = np.zeros(shape=(len(raw_data[0])), dtype=np.longdouble)\n",
    "    DeviationArr = np.zeros(shape=(len(raw_data[0])), dtype=np.longdouble)\n",
    "\n",
    "    for i in range(len(raw_data[0])):\n",
    "        Expected = raw_data[:, i].mean()\n",
    "\n",
    "        shiftedCol = raw_data[:, i] - Expected\n",
    "        Deviation = np.sqrt(np.square(shiftedCol).mean())\n",
    "\n",
    "        res[:, i] = shiftedCol / Deviation if Deviation > 0 else shiftedCol\n",
    "\n",
    "        ExpectedArr[i] = Expected\n",
    "        DeviationArr[i] = Deviation\n",
    "\n",
    "    return res, ExpectedArr, DeviationArr\n",
    "\n",
    "\n",
    "normalized_data, ExpectedArr, DeviationArr = normalize(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create the matrix X, and vector y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = np.ones((len(normalized_data), 1), dtype=np.longdouble)\n",
    "\n",
    "X = np.concatenate((ones, normalized_data[:, :-1]), axis=1)\n",
    "y = normalized_data[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function that recieves the vector $\\Theta$ and a vector $x$, and returns the result of $h_\\Theta(x)$\\\n",
    "We are assuming that $x[0] = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hTheta(theta: np.ndarray, x: np.ndarray):\n",
    "    return (theta * x).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function that recieves a vector $\\Theta$, the matrix $X$, and the vector $y$, and returns the value of $J(\\Theta)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(theta: np.ndarray, X: np.ndarray, y: np.ndarray):\n",
    "    return np.square(np.linalg.norm(X.dot(theta) - y)) / (2 * len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function that recieves a vector $\\Theta$, the matrix $X$, and the vector $y$, and returns the value of $\\triangledown J(\\Theta)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gradient(theta: np.ndarray, X: np.ndarray, y: np.ndarray):\n",
    "    return X.transpose().dot((X.dot(theta) - y)) / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write the Gradient Decent function here.\\\n",
    "We must recieve the data, including X and y, and we also must recieve parameters for our 3 ending conditions:\n",
    "$$||\\Theta^{(k + 1)} - \\Theta^{(k)}|| < \\epsilon$$\n",
    "$$||J(\\Theta^{(k + 1)}) - J(\\Theta^{(k)}))|| < \\delta $$\n",
    "$$k + 1 < M $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GradientDecent(X, y, alpha, epsilon, delta, M):\n",
    "    thetai = np.zeros((len(X[0])))\n",
    "    thetaIPlus = 0, 0, 0\n",
    "\n",
    "    for _ in range(M):\n",
    "        thetaIPlus = thetai - Gradient(thetai, X, y) * alpha\n",
    "        if np.linalg.norm(thetaIPlus - thetai) < epsilon:\n",
    "            break\n",
    "        if np.abs(MSE(thetaIPlus, X, y) - MSE(thetai, X, y)) < delta:\n",
    "            break\n",
    "        thetai = thetaIPlus\n",
    "\n",
    "    return thetaIPlus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the GradientDecent function with multiple alpha values (using M=1000):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.38903820e+16,  1.04315693e+32,  1.05353324e+32,  8.21473438e+30,\n",
       "        4.90964316e+31,  1.05837238e+32, -2.40405681e+31, -8.34917216e+30,\n",
       "       -4.03217349e+31, -4.16107563e+31])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epsilon = 0.000000001\n",
    "delta = epsilon\n",
    "alpha = 1\n",
    "M = 100\n",
    "\n",
    "GradientDecent(X, y, alpha, epsilon, delta, M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we use $\\alpha = 1$, the larger our M, the larger our $\\Theta$ values get to the point of overflowing with a large enough M.\\\n",
    "So, $\\alpha = 1$ does not converge on an answer, instead it diverges, meaning it is too big for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.39377989e-15, -2.84032219e-01,  4.23652633e-01,  4.47311047e-01,\n",
       "       -2.02455830e-01, -2.18094411e-01,  2.41824487e-01, -2.22091361e-03,\n",
       "       -7.32317783e-03,  1.53754788e-02])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = 0.1\n",
    "M = 2250\n",
    "\n",
    "GradientDecent(X, y, alpha, epsilon, delta, M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we use $\\alpha = 0.1$, it seems our answer converges on an answer after around 2250 iterations.\n",
    "I am assuming that our $\\epsilon$ or $\\delta$ conditions are met."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.39442627e-15, -2.83102208e-01,  4.15759922e-01,  4.47553428e-01,\n",
       "       -2.02596593e-01, -2.11037603e-01,  2.41779854e-01, -2.24661823e-03,\n",
       "       -7.60235798e-03,  1.58558664e-02])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = 0.01\n",
    "M = 16900\n",
    "\n",
    "GradientDecent(X, y, alpha, epsilon, delta, M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using $\\alpha = 0.01$, it seems our answer is a bit different, probably more accurate.\\\n",
    "But we are reaching our ending conditions only after around 16900 iterations.\\\n",
    "That means it takes much longer to converge on an answer, yet it is a bit more accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.39462726e-15, -2.78911122e-01,  3.81054361e-01,  4.48614417e-01,\n",
       "       -2.03209377e-01, -1.80107499e-01,  2.41602278e-01, -2.35918520e-03,\n",
       "       -8.74892708e-03,  1.78860456e-02])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = 0.001\n",
    "M = 100000\n",
    "\n",
    "GradientDecent(X, y, alpha, epsilon, delta, M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using $\\alpha = 0.001$, it seems our answer can again get a bit more accurate.\\\n",
    "But also, it seems that to reach a more accurate result, we need to increase our M by a lot.\\\n",
    "To the point that even after increasing M over 100,000, it seems we didn't yet reach our $\\epsilon$ or $\\delta$ ending conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, it seems that we can get better and more accurate results the smaller our $\\alpha$ is.\\\n",
    "But also that means we will need a lot more time to do so.\\\n",
    "Notice, I used a pretty small value of $\\epsilon$ and $\\delta$.\\\n",
    "When using bigger values of $\\epsilon$ and $\\delta$, we will converge on the same answer with smaller $\\alpha$ values, even though it will take longed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
